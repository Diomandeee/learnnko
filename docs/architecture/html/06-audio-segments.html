<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Segmentation - LearnN'Ko Architecture</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
</head>
<body>
    <nav class="sidebar">
        <div class="logo">
            <span class="logo-icon">ß’</span>
            <span>LearnN'Ko</span>
        </div>
        <ul class="nav-links">
            <li><a href="01-data-flow.html" class="">Data Flow</a></li>
<li><a href="02-database-schema.html" class="">Database Schema</a></li>
<li><a href="03-pipeline-passes.html" class="">Pipeline Passes</a></li>
<li><a href="04-frame-extraction.html" class="">Frame Extraction</a></li>
<li><a href="05-world-generation.html" class="">World Generation</a></li>
<li><a href="06-audio-segments.html" class="active">Audio Segments</a></li>
<li><a href="07-resume-mechanism.html" class="">Resume Mechanism</a></li>
<li><a href="README-BUILD.html" class="">Readme Build</a></li>
<li><a href="README.html" class="">Readme</a></li>
        </ul>
        <div class="build-info">
            <small>Built: 2025-12-31 09:27</small>
        </div>
    </nav>
    <main class="content">
        <article>
            <h1>Audio Segmentation</h1>
<p>This document describes the audio extraction and segmentation system for future ASR integration.</p>
<h2>Purpose</h2>
<p>N'Ko educational videos contain spoken explanations alongside visual slides. By extracting and segmenting audio:</p>
<p>1. <strong>Align speech to slides</strong> - Link what's spoken to what's shown
2. <strong>Enable transcription</strong> - Future ASR to get spoken text
3. <strong>Build curriculum</strong> - Audio explanations paired with visual content
4. <strong>Pronunciation training</strong> - Native speaker audio for learners</p>
<h2>Architecture</h2>
<pre><code class="language-">Video File
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AudioExtractor                                â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Extract Full   â”‚â”€â”€â”€â–¶â”‚ Detect Scene   â”‚â”€â”€â”€â–¶â”‚ Slice Audio   â”‚  â”‚
â”‚  â”‚ Audio Track    â”‚    â”‚ Timestamps     â”‚    â”‚ by Scene      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                      â”‚                     â”‚          â”‚
â”‚         â–¼                      â–¼                     â–¼          â”‚
â”‚    audio.m4a            Scene timestamps      segment_<em>.m4a     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
Video Manifest (JSON)</code></pre>
<h2>Process</h2>
<h3>Step 1: Extract Full Audio</h3>
<pre><code class="language-python">def extract_full_audio(video_path: str, output_path: str) -> str:
    """Extract audio track from video."""
    cmd = [
        "ffmpeg",
        "-i", video_path,
        "-vn",                    # No video
        "-c:a", "aac",            # AAC codec
        "-b:a", "128k",           # 128 kbps bitrate
        "-ar", "16000",           # 16kHz sample rate (speech-optimized)
        "-ac", "1",               # Mono
        "-map_metadata", "-1",    # Remove metadata
        "-y",                     # Overwrite
        output_path
    ]
    subprocess.run(cmd, check=True)
    return output_path</code></pre>
<strong>Output:</strong> <code>audio.m4a</code> (~5-10 MB for 60 min video)
<h3>Step 2: Detect Scene Timestamps</h3>
<p>Scene timestamps from frame extraction are reused:</p>
<pre><code class="language-python">def get_scene_timestamps(video_path: str, threshold: float = 0.3) -> List[float]:
    """Detect scene changes in video."""
    # Same scene detection used for frame extraction
    return SceneChangeDetector(threshold).detect_scenes(video_path)</code></pre>
<strong>Output:</strong> <code>[0.0, 45.2, 89.7, 134.1, ...]</code> (seconds)
<h3>Step 3: Slice Audio by Scene</h3>
<pre><code class="language-python">def slice_audio_segments(
    full_audio_path: str,
    output_dir: str,
    scene_timestamps: List[float],
    video_duration_ms: int,
) -> List[SceneInfo]:
    """Slice audio into scene-aligned segments."""
    
    segments = []
    
    # Add start and end boundaries
    timestamps = [0.0] + scene_timestamps + [video_duration_ms / 1000.0]
    
    for i in range(len(timestamps) - 1):
        start_s = timestamps[i]
        end_s = timestamps[i + 1]
        
        segment_path = f"{output_dir}/segment_{i+1:04d}.m4a"
        
        cmd = [
            "ffmpeg",
            "-i", full_audio_path,
            "-ss", str(start_s),          # Start time
            "-t", str(end_s - start_s),   # Duration
            "-c", "copy",                  # No re-encoding
            "-y",
            segment_path
        ]
        subprocess.run(cmd, check=True)
        
        segments.append(SceneInfo(
            index=i + 1,
            start_ms=int(start_s </em> 1000),
            end_ms=int(end_s * 1000),
            audio_path=segment_path,
        ))
    
    return segments</code></pre>
<strong>Output:</strong> <code>segment_0001.m4a</code>, <code>segment_0002.m4a</code>, etc.
<h2>Data Structures</h2>
<pre><code class="language-python">@dataclass
class SceneInfo:
    """Metadata for an audio segment."""
    index: int                           # Segment number (1-based)
    start_ms: int                        # Start timestamp
    end_ms: int                          # End timestamp
    duration_ms: int                     # Segment duration
    frame_path: Optional[str] = None     # Associated frame
    audio_path: Optional[str] = None     # Path to audio file
    has_nko: bool = False                # Frame has N'Ko text
    nko_text: Optional[str] = None       # Detected N'Ko
    latin_text: Optional[str] = None     # Transliteration
    english_text: Optional[str] = None   # Translation
    confidence: float = 0.0              # Detection confidence
    transcription: Optional[str] = None  # Future: ASR result
<p>@dataclass
class VideoManifest:
    """Complete manifest for a processed video."""
    video_id: str
    title: str
    youtube_url: str
    channel_name: Optional[str]
    duration_ms: int
    created_at: str
    scenes: List[SceneInfo]
    
    def to_json(self) -> str:
        return json.dumps(asdict(self), indent=2)</code></pre></p>
<h2>File Structure</h2>
<pre><code class="language-">data/videos/{video_id}/
â”œâ”€â”€ audio.m4a                  # Full audio track
â”œâ”€â”€ audio_segments/            # Scene-based segments
â”‚   â”œâ”€â”€ segment_0001.m4a       # Scene 1 audio
â”‚   â”œâ”€â”€ segment_0002.m4a       # Scene 2 audio
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frames/                    # Extracted frames
â”‚   â”œâ”€â”€ frame_0001.jpg         # Scene 1 frame
â”‚   â”œâ”€â”€ frame_0002.jpg         # Scene 2 frame
â”‚   â””â”€â”€ ...
â””â”€â”€ manifest.json              # All metadata</code></pre>
<h2>Manifest Example</h2>
<pre><code class="language-json">{
  "video_id": "xsUrdpKD5wM",
  "title": "N'Ko Lesson 1 - Greetings",
  "youtube_url": "https://www.youtube.com/watch?v=xsUrdpKD5wM",
  "channel_name": "babamamadidiane",
  "duration_ms": 3522000,
  "created_at": "2024-12-31T08:45:00.000Z",
  "scenes": [
    {
      "index": 1,
      "start_ms": 0,
      "end_ms": 45200,
      "duration_ms": 45200,
      "frame_path": "frames/frame_0001.jpg",
      "audio_path": "audio_segments/segment_0001.m4a",
      "has_nko": true,
      "nko_text": "ß’ ß›ßß° ßßŠß²ß¬",
      "latin_text": "N so kan",
      "english_text": "My greetings",
      "confidence": 0.95,
      "transcription": null
    },
    {
      "index": 2,
      "start_ms": 45200,
      "end_ms": 89700,
      ...
    }
  ]
}</code></pre>
<h2>Database Storage</h2>
<pre><code class="language-sql">-- nko_audio_segments table
CREATE TABLE nko_audio_segments (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_id UUID REFERENCES nko_sources(id),
    frame_id UUID REFERENCES nko_frames(id),
    segment_index INT NOT NULL,
    start_ms BIGINT NOT NULL,
    end_ms BIGINT NOT NULL,
    audio_path TEXT,
    transcription TEXT,
    transcription_confidence FLOAT,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT now()
);</code></pre>
<h2>Configuration</h2>
<pre><code class="language-yaml"><h1>config/production.yaml</h1>
audio:
  format: "m4a"           # Output format
  bitrate: "128k"         # Audio quality
  sample_rate: 16000      # Hz (speech-optimized)
  channels: 1             # Mono
<p>storage:
  local:
    keep_audio: true      # Don't delete after processing</code></pre></p>
<h2>Future: ASR Integration</h2>
<p>When ASR (Pass 4) is implemented:</p>
<pre><code class="language-python">async def transcribe_segment(audio_path: str) -> Transcription:
    """Transcribe an audio segment using Whisper."""
    
    # Option 1: OpenAI Whisper API
    with open(audio_path, "rb") as audio:
        response = await openai.audio.transcriptions.create(
            model="whisper-1",
            file=audio,
            language="nqo",  # N'Ko language code
        )
    
    return Transcription(
        text=response.text,
        confidence=response.confidence,
    )</code></pre>
<h3>Curriculum Building</h3>
<p>With transcriptions:</p>
<pre><code class="language-">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Slide: ß’ ß›ßß° ßßŠß²ß¬ (N so kan - My greetings)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  ğŸ–¼ï¸ Visual:   [Slide showing N'Ko text]                         â”‚
â”‚                                                                  â”‚
â”‚  ğŸ”Š Audio:    "This phrase 'N so kan' is used when..."          â”‚
â”‚               (45 seconds of explanation)                        â”‚
â”‚                                                                  â”‚
â”‚  ğŸ“ Text:     [Full transcription of explanation]                â”‚
â”‚                                                                  â”‚
â”‚  âœ¨ Worlds:   [5 contextual examples]                            â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
<h2>Cost Estimate</h2>
<table>
<thead>
<tr>
<th>Component</th>
<th>Calculation</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Audio extraction</td>
<td>FFmpeg (local)</td>
<td>$0</td>
</tr>
<tr>
<td>Audio storage</td>
<td>~500 MB total</td>
<td>~$0.01/month</td>
</tr>
<tr>
<td>Whisper API</td>
<td>522 Ã— 60 min Ã— $0.006</td>
<td>$188</td>
</tr>
</tbody>
</table>
<strong>Without ASR: FREE</strong>
<strong>With ASR: ~$188</strong>

        </article>
    </main>
    <script>
        mermaid.initialize({ 
            startOnLoad: true, 
            theme: 'dark',
            themeVariables: {
                primaryColor: '#00d4ff',
                primaryTextColor: '#fff',
                primaryBorderColor: '#00d4ff',
                lineColor: '#a855f7',
                secondaryColor: '#1e1e2e',
                tertiaryColor: '#2d2d44'
            }
        });
    </script>
</body>
</html>
